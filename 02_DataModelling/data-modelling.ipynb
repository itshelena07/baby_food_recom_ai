{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "939fdc71",
   "metadata": {},
   "source": [
    "<h3><strong>Data Modelling</strong></h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d81c315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent directory: c:\\Users\\Helena\\Desktop\\APU\\Semester 5\\INV\\IR - PROPOSAL\\scraping-code\\baby-food-recom-data-ai\n",
      "Full file path: c:\\Users\\Helena\\Desktop\\APU\\Semester 5\\INV\\IR - PROPOSAL\\scraping-code\\baby-food-recom-data-ai\\01_DataPreprocessing\\current_dataset.xlsx\n",
      "File exists: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "print(f\"Parent directory: {parent_dir}\")\n",
    "\n",
    "file_path = os.path.join(parent_dir, '01_DataPreprocessing', 'current_dataset.xlsx')\n",
    "print(f\"Full file path: {file_path}\")\n",
    "print(f\"File exists: {os.path.exists(file_path)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5e5d28",
   "metadata": {},
   "source": [
    "**Feature Engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8ab2f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully opened the Excel file!\n"
     ]
    }
   ],
   "source": [
    "# If file exists, open it\n",
    "import pandas as pd\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    import openpyxl\n",
    "    workbook = openpyxl.load_workbook(file_path)\n",
    "    worksheet = workbook[\"Sheet1\"]\n",
    "    print(\"Successfully opened the Excel file!\")\n",
    "\n",
    "df = pd.read_excel(file_path)\n",
    "df[['choking_hazard', 'choking_hazards']].head()\n",
    "#drop\n",
    "df.drop(columns=['choking_hazard'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "201084d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'food_name' is complete.\n",
      "Column 'ingredient' is complete.\n",
      "Column 'ner_ingredient' is complete.\n",
      "Column 'instructions' is complete.\n",
      "Column 'recipe_link' is complete.\n"
     ]
    }
   ],
   "source": [
    "#drop data if imporant columns are empty\n",
    "important_columns = ['food_name', 'ingredient', 'instructions',  'ner_ingredient','recipe_link']\n",
    "for col in df.columns:\n",
    "    if col in important_columns:\n",
    "        null_count = df[col].isnull().sum()        \n",
    "        if null_count >0:\n",
    "            print(f\"Column '{col}' has {null_count} null values.\")  \n",
    "            # df = df.dropna(subset=[col])\n",
    "        else:\n",
    "            print(f\"Column '{col}' is complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "faf6b939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 520 entries, 0 to 519\n",
      "Data columns (total 25 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   food_name        520 non-null    object \n",
      " 1   ingredient       520 non-null    object \n",
      " 2   ner_ingredient   520 non-null    object \n",
      " 3   instructions     520 non-null    object \n",
      " 4   min_age_group    520 non-null    int64  \n",
      " 5   max_age_group    520 non-null    int64  \n",
      " 6   texture          520 non-null    object \n",
      " 7   prep_time        388 non-null    float64\n",
      " 8   cook_time        363 non-null    float64\n",
      " 9   serving          439 non-null    object \n",
      " 10  origin           466 non-null    object \n",
      " 11  recipe_link      520 non-null    object \n",
      " 12  credibility      246 non-null    object \n",
      " 13  image_link       42 non-null     object \n",
      " 14  region           466 non-null    object \n",
      " 15  flag_code        466 non-null    object \n",
      " 16  difficulty       520 non-null    object \n",
      " 17  meal_type        169 non-null    object \n",
      " 18  description      108 non-null    object \n",
      " 19  dietary_tags     520 non-null    object \n",
      " 20  tips             21 non-null     object \n",
      " 21  allergen         520 non-null    object \n",
      " 22  hypoallergenic   520 non-null    object \n",
      " 23  nutrition_value  241 non-null    object \n",
      " 24  choking_hazards  520 non-null    object \n",
      "dtypes: float64(2), int64(2), object(21)\n",
      "memory usage: 101.7+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(\"DataFrame info:\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4375a56a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in each column:\n",
      "--------------------------------------------------\n",
      "food_name: 518 unique values\n",
      "--------------------------------------------------\n",
      "ingredient: 514 unique values\n",
      "--------------------------------------------------\n",
      "ner_ingredient: 508 unique values\n",
      "--------------------------------------------------\n",
      "instructions: 518 unique values\n",
      "--------------------------------------------------\n",
      "min_age_group: [ 7 12  6  9 10  8]\n",
      "Value counts for min_age_group:\n",
      "min_age_group\n",
      "6     235\n",
      "12    106\n",
      "9     104\n",
      "7      40\n",
      "10     23\n",
      "8      12\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------------------\n",
      "max_age_group: [ 8 12 24  9  6 36  7 11 23]\n",
      "Value counts for max_age_group:\n",
      "max_age_group\n",
      "12    103\n",
      "24    101\n",
      "9      80\n",
      "8      70\n",
      "6      70\n",
      "11     61\n",
      "36     25\n",
      "7       6\n",
      "23      4\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------------------\n",
      "texture: ['puree' 'NONE' 'lumpy texture' 'family food' 'soft finger food']\n",
      "Value counts for texture:\n",
      "texture\n",
      "puree               247\n",
      "NONE                178\n",
      "family food          71\n",
      "lumpy texture        21\n",
      "soft finger food      3\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------------------\n",
      "prep_time: [10.  5.  2. nan 15. 20. 30. 45.]\n",
      "Value counts for prep_time:\n",
      "prep_time\n",
      "10.0    160\n",
      "15.0    125\n",
      "20.0     49\n",
      "5.0      44\n",
      "2.0       5\n",
      "30.0      3\n",
      "45.0      2\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------------------\n",
      "cook_time: 20 unique values\n",
      "--------------------------------------------------\n",
      "serving: 48 unique values\n",
      "--------------------------------------------------\n",
      "origin: 18 unique values\n",
      "--------------------------------------------------\n",
      "recipe_link: 300 unique values\n",
      "--------------------------------------------------\n",
      "credibility: [nan 'Dina DiMaggio, M.D.' 'KlikDocter Medical Team' 'Annabel Karmel'\n",
      " 'France Lait Laboratory' 'UK National Health Service']\n",
      "Value counts for credibility:\n",
      "credibility\n",
      "France Lait Laboratory        115\n",
      "UK National Health Service     98\n",
      "Dina DiMaggio, M.D.            30\n",
      "Annabel Karmel                  2\n",
      "KlikDocter Medical Team         1\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------------------\n",
      "image_link: 43 unique values\n",
      "--------------------------------------------------\n",
      "region: ['Uncategorized' 'Asian' 'American' 'Southeast Asian' 'Indian' nan\n",
      " 'Oceanic']\n",
      "Value counts for region:\n",
      "region\n",
      "Uncategorized      254\n",
      "Southeast Asian    163\n",
      "American            30\n",
      "Oceanic             14\n",
      "Asian                4\n",
      "Indian               1\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------------------\n",
      "flag_code: ['unknown' 'jp' 'us' 'id' 'bt' nan 'nz']\n",
      "Value counts for flag_code:\n",
      "flag_code\n",
      "unknown    254\n",
      "id         163\n",
      "us          30\n",
      "nz          14\n",
      "jp           4\n",
      "bt           1\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------------------\n",
      "difficulty: ['Medium' 'Easy' 'Hard']\n",
      "Value counts for difficulty:\n",
      "difficulty\n",
      "Medium    251\n",
      "Easy      140\n",
      "Hard      129\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------------------\n",
      "meal_type: [nan 'Meal' 'Dessert' 'Drink' 'Dinner' 'Breakfast' 'Breakfast, Lunch'\n",
      " 'Snack']\n",
      "Value counts for meal_type:\n",
      "meal_type\n",
      "Meal                74\n",
      "Dessert             40\n",
      "Dinner              38\n",
      "Breakfast           12\n",
      "Snack                3\n",
      "Drink                1\n",
      "Breakfast, Lunch     1\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------------------\n",
      "description: 104 unique values\n",
      "--------------------------------------------------\n",
      "dietary_tags: 42 unique values\n",
      "--------------------------------------------------\n",
      "tips: 22 unique values\n",
      "--------------------------------------------------\n",
      "allergen: 31 unique values\n",
      "--------------------------------------------------\n",
      "hypoallergenic: ['No' 'Yes']\n",
      "Value counts for hypoallergenic:\n",
      "hypoallergenic\n",
      "No     336\n",
      "Yes    184\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------------------\n",
      "nutrition_value: 238 unique values\n",
      "--------------------------------------------------\n",
      "choking_hazards: ['No' 'Yes']\n",
      "Value counts for choking_hazards:\n",
      "choking_hazards\n",
      "No     517\n",
      "Yes      3\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Get unique values for each column\n",
    "print(\"Unique values in each column:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for column in df.columns:\n",
    "    unique_values = df[column].unique()\n",
    "    \n",
    "    # Get the count of unique values\n",
    "    unique_count = len(unique_values)\n",
    "    \n",
    "    # For columns with many unique values, just show the count\n",
    "    if unique_count > 10:\n",
    "        print(f\"{column}: {unique_count} unique values\")\n",
    "    else:\n",
    "        # For columns with few unique values, show all values\n",
    "        print(f\"{column}: {unique_values}\")\n",
    "    \n",
    "    # Show value counts for categorical columns with few unique values\n",
    "    if unique_count <= 10 and unique_count > 1:\n",
    "        print(f\"Value counts for {column}:\")\n",
    "        print(df[column].value_counts())\n",
    "    \n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8472422",
   "metadata": {},
   "source": [
    "*Cosine*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f809bc",
   "metadata": {},
   "source": [
    "Using TF-IDF --> use ner ingredients, ingredients, instructions, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b5a22ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MultiLabelBinarizer, LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bc4d7c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allergen_list:\n",
      "0                  [soy]\n",
      "1                     []\n",
      "2    [milk, soy, gluten]\n",
      "3           [milk, fish]\n",
      "4                  [soy]\n",
      "Name: allergen_list, dtype: object\n",
      "allergen_str:\n",
      "0                soy\n",
      "1                   \n",
      "2    milk soy gluten\n",
      "3          milk fish\n",
      "4                soy\n",
      "Name: allergen_str, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "# Convert stringified list to actual list\n",
    "df['allergen_list'] = df['allergen'].apply(ast.literal_eval)\n",
    "\n",
    "# Handle empty lists by checking if the list is empty\n",
    "df['allergen_str'] = df['allergen_list'].apply(lambda x: ' '.join(x) if x else '')\n",
    "\n",
    "print(\"allergen_list:\")\n",
    "print(df['allergen_list'].head()) \n",
    "print(\"allergen_str:\")\n",
    "print(df['allergen_str'].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d13edb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dietary_list:\n",
      "0    [vegan, vegetarian, pescetarian, dairy_free, e...\n",
      "1    [pescetarian, dairy_free, egg_free, soy_free, ...\n",
      "2    [vegetarian, pescetarian, egg_free, nut_free, ...\n",
      "3    [pescetarian, egg_free, soy_free, nut_free, gl...\n",
      "4    [vegan, vegetarian, pescetarian, dairy_free, e...\n",
      "Name: dietary_list, dtype: object\n",
      "dietary_str:\n",
      "0    vegan vegetarian pescetarian dairy_free egg_fr...\n",
      "1    pescetarian dairy_free egg_free soy_free nut_f...\n",
      "2    vegetarian pescetarian egg_free nut_free halal...\n",
      "3    pescetarian egg_free soy_free nut_free gluten_...\n",
      "4    vegan vegetarian pescetarian dairy_free egg_fr...\n",
      "Name: dietary_str, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df['dietary_list'] = df['dietary_tags'].apply(ast.literal_eval)\n",
    "\n",
    "# Handle empty lists by checking if the list is empty\n",
    "df['dietary_str'] = df['dietary_list'].apply(lambda x: ' '.join(x) if x else '')\n",
    "\n",
    "print(\"dietary_list:\")\n",
    "print(df['dietary_list'].head()) \n",
    "print(\"dietary_str:\")\n",
    "print(df['dietary_str'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7ff13e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated dietary tag columns: ['dairy free', 'egg free', 'gluten free', 'halal', 'non halal', 'non veg', 'nut free', 'pescetarian', 'soy free', 'vegan', 'vegetarian']\n",
      "Dietary tags shape: (520, 11)\n",
      "Dietary tag categories: ['dairy free', 'egg free', 'gluten free', 'halal', 'non halal', 'non veg', 'nut free', 'pescetarian', 'soy free', 'vegan', 'vegetarian']\n",
      "Allergen shape: (520, 8)\n",
      "Allergen categories: ['egg', 'fish', 'gluten', 'milk', 'nuts', 'peanuts', 'shellfish', 'soy']\n",
      "\n",
      "Sample dietary tags data:\n",
      "   dairy free  egg free  gluten free  halal  non halal  non veg  nut free  \\\n",
      "0           1         1            1      1          0        1         1   \n",
      "1           1         1            1      1          0        1         1   \n",
      "2           0         1            0      1          0        1         1   \n",
      "3           0         1            1      1          0        1         1   \n",
      "4           1         1            1      1          0        1         1   \n",
      "\n",
      "   pescetarian  soy free  vegan  vegetarian  \n",
      "0            1         0      1           1  \n",
      "1            1         1      0           0  \n",
      "2            1         0      0           1  \n",
      "3            1         1      0           0  \n",
      "4            1         0      1           1  \n",
      "\n",
      "Sample allergen data:\n",
      "   egg  fish  gluten  milk  nuts  peanuts  shellfish  soy\n",
      "0    0     0       0     0     0        0          0    1\n",
      "1    0     0       0     0     0        0          0    0\n",
      "2    0     0       1     1     0        0          0    1\n",
      "3    0     1       0     1     0        0          0    0\n",
      "4    0     0       0     0     0        0          0    1\n"
     ]
    }
   ],
   "source": [
    "# Multi-hot encoding for multi-label columns\n",
    "mlb_tags = MultiLabelBinarizer()\n",
    "tags_df = pd.DataFrame(mlb_tags.fit_transform(df['dietary_list']), columns=mlb_tags.classes_)\n",
    "\n",
    "# Transform dietary tag column names: replace underscores with spaces for better readability\n",
    "tags_df.columns = [col.replace('_', ' ') for col in tags_df.columns]\n",
    "print(\"Updated dietary tag columns:\", list(tags_df.columns))\n",
    "\n",
    "mlb_allergen = MultiLabelBinarizer()\n",
    "allergen_df = pd.DataFrame(mlb_allergen.fit_transform(df['allergen_list']), columns=mlb_allergen.classes_)\n",
    "\n",
    "# Print the shapes to verify\n",
    "print(f\"Dietary tags shape: {tags_df.shape}\")\n",
    "print(f\"Dietary tag categories: {list(tags_df.columns)}\")\n",
    "print(f\"Allergen shape: {allergen_df.shape}\")\n",
    "print(f\"Allergen categories: {list(allergen_df.columns)}\")\n",
    "\n",
    "# Show some sample data\n",
    "print(\"\\nSample dietary tags data:\")\n",
    "print(tags_df.head())\n",
    "print(\"\\nSample allergen data:\")\n",
    "print(allergen_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a9c2984c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>choking_hazards</th>\n",
       "      <th>hypoallergenic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   choking_hazards  hypoallergenic\n",
       "0              0.0             0.0\n",
       "1              0.0             0.0\n",
       "2              0.0             0.0\n",
       "3              0.0             0.0\n",
       "4              0.0             0.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['choking_hazards', 'hypoallergenic']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5bb9f246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data - Unique values in choking_hazards:\n",
      "['No' 'Yes']\n",
      "\n",
      "Choking hazards value counts:\n",
      "choking_hazards\n",
      "No     517\n",
      "Yes      3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Original data - Unique values in hypoallergenic:\n",
      "['No' 'Yes']\n",
      "\n",
      "Hypoallergenic value counts:\n",
      "hypoallergenic\n",
      "No     336\n",
      "Yes    184\n",
      "Name: count, dtype: int64\n",
      "\n",
      "All columns containing 'chok' or 'hazard':\n",
      "['choking_hazard', 'choking_hazards']\n",
      "\n",
      "All columns containing 'hypo' or 'allerg':\n",
      "['allergen', 'hypoallergenic']\n",
      "\n",
      "Sample of original data:\n",
      "  choking_hazard choking_hazards                   allergen hypoallergenic\n",
      "0             No              No                    ['soy']             No\n",
      "1             No              No                         []            Yes\n",
      "2             No              No  ['milk', 'soy', 'gluten']             No\n",
      "3            Yes             Yes           ['milk', 'fish']             No\n",
      "4             No              No                    ['soy']             No\n",
      "5             No              No                         []            Yes\n",
      "6             No              No                         []            Yes\n",
      "7             No              No                         []            Yes\n",
      "8             No              No                   ['milk']             No\n",
      "9             No              No                   ['milk']             No\n",
      "Original values before mapping:\n",
      "Unique values in choking_hazards: ['No' 'Yes']\n",
      "Unique values in hypoallergenic: ['No' 'Yes']\n",
      "\n",
      "After mapping:\n",
      "Unique values in choking_hazards: [0. 1.]\n",
      "Unique values in hypoallergenic: [0. 1.]\n",
      "\n",
      "Value counts after mapping:\n",
      "Choking hazards:\n",
      "choking_hazards\n",
      "0.0    517\n",
      "1.0      3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Hypoallergenic:\n",
      "hypoallergenic\n",
      "0.0    336\n",
      "1.0    184\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample of mapped data:\n",
      "   hypoallergenic  choking_hazards\n",
      "0             0.0              0.0\n",
      "1             1.0              0.0\n",
      "2             0.0              0.0\n",
      "3             0.0              1.0\n",
      "4             0.0              0.0\n",
      "5             1.0              0.0\n",
      "6             1.0              0.0\n",
      "7             1.0              0.0\n",
      "8             0.0              0.0\n",
      "9             0.0              0.0\n"
     ]
    }
   ],
   "source": [
    "# Let's reload the original data to check the raw values\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Reload the original Excel file\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "file_path = os.path.join(parent_dir, '01_DataPreprocessing', 'current_dataset.xlsx')\n",
    "\n",
    "# Read fresh data\n",
    "original_df = pd.read_excel(file_path)\n",
    "\n",
    "print(\"Original data - Unique values in choking_hazards:\")\n",
    "if 'choking_hazards' in original_df.columns:\n",
    "    print(original_df['choking_hazards'].unique())\n",
    "    print(\"\\nChoking hazards value counts:\")\n",
    "    print(original_df['choking_hazards'].value_counts(dropna=False))\n",
    "else:\n",
    "    print(\"choking_hazards column not found\")\n",
    "\n",
    "print(\"\\nOriginal data - Unique values in hypoallergenic:\")\n",
    "if 'hypoallergenic' in original_df.columns:\n",
    "    print(original_df['hypoallergenic'].unique())\n",
    "    print(\"\\nHypoallergenic value counts:\")\n",
    "    print(original_df['hypoallergenic'].value_counts(dropna=False))\n",
    "else:\n",
    "    print(\"hypoallergenic column not found\")\n",
    "\n",
    "# Check if there are other similar columns\n",
    "print(\"\\nAll columns containing 'chok' or 'hazard':\")\n",
    "chok_cols = [col for col in original_df.columns if 'chok' in col.lower() or 'hazard' in col.lower()]\n",
    "print(chok_cols)\n",
    "\n",
    "print(\"\\nAll columns containing 'hypo' or 'allerg':\")\n",
    "hypo_cols = [col for col in original_df.columns if 'hypo' in col.lower() or 'allerg' in col.lower()]\n",
    "print(hypo_cols)\n",
    "\n",
    "# Show sample of the original data\n",
    "print(\"\\nSample of original data:\")\n",
    "if chok_cols or hypo_cols:\n",
    "    relevant_cols = chok_cols + hypo_cols\n",
    "    print(original_df[relevant_cols].head(10))\n",
    "else:\n",
    "    print(\"No relevant columns found\")\n",
    "\n",
    "# First reload the original data to ensure we have the correct values\n",
    "# Reload the original Excel file to get fresh data\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Drop the duplicate choking_hazard column\n",
    "if 'choking_hazard' in df.columns:\n",
    "    df.drop(columns=['choking_hazard'], inplace=True)\n",
    "\n",
    "print(\"Original values before mapping:\")\n",
    "print(\"Unique values in choking_hazards:\", df['choking_hazards'].unique())\n",
    "print(\"Unique values in hypoallergenic:\", df['hypoallergenic'].unique())\n",
    "\n",
    "# Correct column mapping with proper case (Yes/No instead of yes/no)\n",
    "df['choking_hazards'] = df['choking_hazards'].map({'Yes': 1, 'No': 0}).fillna(0)\n",
    "df['choking_hazards'] = df['choking_hazards'].astype(float)\n",
    "df['hypoallergenic'] = df['hypoallergenic'].map({'Yes': 1, 'No': 0}).fillna(0)\n",
    "df['hypoallergenic'] = df['hypoallergenic'].astype(float)\n",
    "\n",
    "print(\"\\nAfter mapping:\")\n",
    "print(\"Unique values in choking_hazards:\", df['choking_hazards'].unique())\n",
    "print(\"Unique values in hypoallergenic:\", df['hypoallergenic'].unique())\n",
    "\n",
    "print(\"\\nValue counts after mapping:\")\n",
    "print(\"Choking hazards:\")\n",
    "print(df['choking_hazards'].value_counts())\n",
    "print(\"\\nHypoallergenic:\")\n",
    "print(df['hypoallergenic'].value_counts())\n",
    "\n",
    "print(\"\\nSample of mapped data:\")\n",
    "print(df[['hypoallergenic', 'choking_hazards']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4481f614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_ingredient_list:\n",
      "0           [sweet potato, edamame, cornstarch, water]\n",
      "1    [onion, chicken breast, udon thin, dashi, clea...\n",
      "2                              [yogurt, soybean flour]\n",
      "3    [japanese rice, natto, long onion, egg, butter...\n",
      "4                            [dashi, tofu, miso paste]\n",
      "Name: ner_ingredient_list, dtype: object\n",
      "ner_ingredient_str:\n",
      "0                sweet potato edamame cornstarch water\n",
      "1    onion chicken breast udon thin dashi clear bro...\n",
      "2                                 yogurt soybean flour\n",
      "3    japanese rice natto long onion egg butter fill...\n",
      "4                                dashi tofu miso paste\n",
      "Name: ner_ingredient_str, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "# Convert stringified list to actual list\n",
    "df['ner_ingredient_list'] = df['ner_ingredient'].apply(ast.literal_eval)\n",
    "df['ner_ingredient_str'] = df['ner_ingredient_list'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "print(\"ner_ingredient_list:\")\n",
    "print(df['ner_ingredient_list'].head()) \n",
    "print(\"ner_ingredient_str:\")\n",
    "print(df['ner_ingredient_str'].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "933856a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert ENGLISH_STOP_WORDS from frozenset to list\n",
    "stop_words_list = list(ENGLISH_STOP_WORDS)\n",
    "\n",
    "# Use the list in TfidfVectorizer\n",
    "df['combined_text'] = (\n",
    "    df['ner_ingredient_str'] + ' ' + df['texture'] )\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=stop_words_list)  # or simply use 'english'\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['combined_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1a45ec52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack, csr_matrix\n",
    "\n",
    "binary_features = csr_matrix(df[['choking_hazards', 'hypoallergenic']].values)\n",
    "\n",
    "final_features = hstack([\n",
    "    tfidf_matrix, \n",
    "    tags_df, \n",
    "    allergen_df,\n",
    "    binary_features\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1b52c277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix shape: (520, 421)\n",
      "Data type: float64\n",
      "Memory usage (bytes): 152624\n",
      "Percentage of non-zero elements: 4.36%\n"
     ]
    }
   ],
   "source": [
    "# Print summary statistics about the sparse matrix\n",
    "print(f\"Matrix shape: {final_features.shape}\")\n",
    "print(f\"Data type: {final_features.dtype}\")\n",
    "print(f\"Memory usage (bytes): {final_features.data.nbytes + final_features.indices.nbytes + final_features.indptr.nbytes if hasattr(final_features, 'indptr') else final_features.data.nbytes + final_features.row.nbytes + final_features.col.nbytes}\")\n",
    "print(f\"Percentage of non-zero elements: {100 * final_features.nnz / (final_features.shape[0] * final_features.shape[1]):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cd4497ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF part for first recipe: [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.43301403 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.68968997 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.28176103 0.         0.         0.\n",
      " 0.         0.         0.18310746 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.40164422 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.25018173\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "Dietary tags part for first recipe: [1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1.]\n",
      "Allergen part for first recipe: [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "Binary features for first recipe: [0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Convert to CSR format first, which supports efficient slicing\n",
    "final_features_csr = final_features.tocsr()\n",
    "\n",
    "# Now you can access the parts you want\n",
    "print(\"TF-IDF part for first recipe:\", final_features_csr[0, :tfidf_matrix.shape[1]].toarray()[0])\n",
    "\n",
    "\n",
    "tags_shape = tags_df.shape[1]\n",
    "print(\"Dietary tags part for first recipe:\", \n",
    "      final_features_csr[0, tfidf_matrix.shape[1]:tfidf_matrix.shape[1]+tags_shape].toarray()[0])\n",
    "print(\"Allergen part for first recipe:\", \n",
    "      final_features_csr[0, tfidf_matrix.shape[1]+tags_shape:tfidf_matrix.shape[1]+tags_shape+allergen_df.shape[1]].toarray()[0])\n",
    "print(\"Binary features for first recipe:\", \n",
    "      final_features_csr[0, -2:].toarray()[0])  # Assuming binary_features has 2 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "64c9308c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF feature names (first 10 of 400):\n",
      "['abon' 'agar' 'almond' 'ambon' 'anchovy' 'apple' 'applesauce' 'apricot'\n",
      " 'aron' 'artichoke']\n",
      "\n",
      "Dietary tag features (11):\n",
      "['dairy_free', 'egg_free', 'gluten_free', 'halal', 'non_halal', 'non_veg', 'nut_free', 'pescetarian', 'soy_free', 'vegan', 'vegetarian']\n",
      "\n",
      "Allergen features (8):\n",
      "['egg', 'fish', 'gluten', 'milk', 'nuts', 'peanuts', 'shellfish', 'soy']\n",
      "\n",
      "Binary features:\n",
      "['choking_hazards', 'hypoallergenic']\n",
      "\n",
      "----- SAMPLE VALUES FOR FIRST RECIPE -----\n",
      "TF-IDF part (showing non-zero values only):\n",
      "  cornstarch: 0.4330\n",
      "  edamame: 0.6897\n",
      "  potato: 0.2818\n",
      "  puree: 0.1831\n",
      "  sweet: 0.4016\n",
      "  water: 0.2502\n",
      "\n",
      "Dietary tags:\n",
      "  dairy_free: 1.0\n",
      "  egg_free: 1.0\n",
      "  gluten_free: 1.0\n",
      "  halal: 1.0\n",
      "  non_veg: 1.0\n",
      "  nut_free: 1.0\n",
      "  pescetarian: 1.0\n",
      "  vegan: 1.0\n",
      "  vegetarian: 1.0\n",
      "\n",
      "Allergen part:\n",
      "  soy: 1.0\n",
      "\n",
      "Binary features:\n",
      "  choking_hazards: 0.0\n",
      "  hypoallergenic: 0.0\n",
      "\n",
      "----- VERIFICATION: CHECKING RECIPES WITH DIFFERENT BINARY VALUES -----\n",
      "\n",
      "Recipes with choking hazards (found 3):\n",
      "Recipe 3: choking_hazards=1.0, hypoallergenic=0.0\n",
      "\n",
      "Recipes with hypoallergenic (found 184):\n",
      "Recipe 1: choking_hazards=0.0, hypoallergenic=1.0\n",
      "\n",
      "Total feature dimensions: TF-IDF=400, Dietary=11, Allergen=8, Binary=2\n",
      "Total features: 421\n",
      "Final matrix shape: (520, 421)\n",
      "\n",
      "----- BINARY FEATURE DISTRIBUTION -----\n",
      "Choking hazards: 3 recipes with hazards out of 520\n",
      "Hypoallergenic: 184 recipes that are hypoallergenic out of 520\n"
     ]
    }
   ],
   "source": [
    "# Convert to CSR format first, which supports efficient slicing\n",
    "import numpy as np\n",
    "\n",
    "final_features_csr = final_features.tocsr()\n",
    "\n",
    "# Get the TF-IDF feature names\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "print(f\"\\nTF-IDF feature names (first 10 of {len(tfidf_feature_names)}):\")\n",
    "print(tfidf_feature_names[:10])\n",
    "\n",
    "# Get the dietary tags feature names\n",
    "dietary_feature_names = list(tags_df.columns)\n",
    "print(f\"\\nDietary tag features ({len(dietary_feature_names)}):\")\n",
    "print(dietary_feature_names)\n",
    "\n",
    "# Get the allergen feature names\n",
    "allergen_feature_names = list(allergen_df.columns)\n",
    "print(f\"\\nAllergen features ({len(allergen_feature_names)}):\")\n",
    "print(allergen_feature_names)\n",
    "\n",
    "# Binary feature names\n",
    "binary_feature_names = ['choking_hazards', 'hypoallergenic']\n",
    "print(f\"\\nBinary features:\")\n",
    "print(binary_feature_names)\n",
    "\n",
    "# Update the shape calculations\n",
    "tags_shape = tags_df.shape[1]\n",
    "allergen_shape = allergen_df.shape[1]\n",
    "binary_shape = 2\n",
    "\n",
    "# Print sample values for first recipe\n",
    "print(\"\\n----- SAMPLE VALUES FOR FIRST RECIPE -----\")\n",
    "print(\"TF-IDF part (showing non-zero values only):\")\n",
    "tfidf_values = final_features_csr[0, :tfidf_matrix.shape[1]].toarray()[0]\n",
    "non_zero_indices = np.nonzero(tfidf_values)[0]\n",
    "for idx in non_zero_indices:\n",
    "    print(f\"  {tfidf_feature_names[idx]}: {tfidf_values[idx]:.4f}\")\n",
    "\n",
    "print(\"\\nDietary tags:\")\n",
    "tags_start = tfidf_matrix.shape[1]\n",
    "tags_end = tags_start + tags_shape\n",
    "tags_values = final_features_csr[0, tags_start:tags_end].toarray()[0]\n",
    "for i, val in enumerate(tags_values):\n",
    "    if val > 0:\n",
    "        print(f\"  {dietary_feature_names[i]}: {val}\")\n",
    "\n",
    "print(\"\\nAllergen part:\")\n",
    "allergen_start = tags_end\n",
    "allergen_end = allergen_start + allergen_shape\n",
    "allergen_values = final_features_csr[0, allergen_start:allergen_end].toarray()[0]\n",
    "for i, val in enumerate(allergen_values):\n",
    "    if val > 0:\n",
    "        print(f\"  {allergen_feature_names[i]}: {val}\")\n",
    "\n",
    "print(\"\\nBinary features:\")\n",
    "binary_values = final_features_csr[0, -binary_shape:].toarray()[0]\n",
    "for i, name in enumerate(binary_feature_names):\n",
    "    print(f\"  {name}: {binary_values[i]}\")\n",
    "\n",
    "# Check samples with different binary values\n",
    "print(\"\\n----- VERIFICATION: CHECKING RECIPES WITH DIFFERENT BINARY VALUES -----\")\n",
    "\n",
    "# Find recipes with choking hazards = 1\n",
    "choking_hazard_indices = df[df['choking_hazards'] == 1].index\n",
    "print(f\"\\nRecipes with choking hazards (found {len(choking_hazard_indices)}):\")\n",
    "if len(choking_hazard_indices) > 0:\n",
    "    idx = choking_hazard_indices[0]\n",
    "    binary_vals = final_features_csr[idx, -binary_shape:].toarray()[0]\n",
    "    print(f\"Recipe {idx}: choking_hazards={binary_vals[0]}, hypoallergenic={binary_vals[1]}\")\n",
    "\n",
    "# Find recipes with hypoallergenic = 1\n",
    "hypoallergenic_indices = df[df['hypoallergenic'] == 1].index\n",
    "print(f\"\\nRecipes with hypoallergenic (found {len(hypoallergenic_indices)}):\")\n",
    "if len(hypoallergenic_indices) > 0:\n",
    "    idx = hypoallergenic_indices[0]\n",
    "    binary_vals = final_features_csr[idx, -binary_shape:].toarray()[0]\n",
    "    print(f\"Recipe {idx}: choking_hazards={binary_vals[0]}, hypoallergenic={binary_vals[1]}\")\n",
    "\n",
    "print(f\"\\nTotal feature dimensions: TF-IDF={tfidf_matrix.shape[1]}, Dietary={tags_shape}, Allergen={allergen_shape}, Binary={binary_shape}\")\n",
    "print(f\"Total features: {tfidf_matrix.shape[1] + tags_shape + allergen_shape + binary_shape}\")\n",
    "print(f\"Final matrix shape: {final_features_csr.shape}\")\n",
    "\n",
    "# Summary of binary feature distribution\n",
    "print(\"\\n----- BINARY FEATURE DISTRIBUTION -----\")\n",
    "print(f\"Choking hazards: {int(df['choking_hazards'].sum())} recipes with hazards out of {len(df)}\")\n",
    "print(f\"Hypoallergenic: {int(df['hypoallergenic'].sum())} recipes that are hypoallergenic out of {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9c1afdec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of features: 421\n",
      "Feature names: ['abon', 'agar', 'almond', 'ambon', 'anchovy', 'apple', 'applesauce', 'apricot', 'aron', 'artichoke']...\n",
      "\n",
      "Feature matrix shape: (520, 421)\n",
      "\n",
      "First few rows of feature matrix:\n",
      "   abon  agar  almond  ambon  anchovy  apple  applesauce  apricot  aron  \\\n",
      "0   0.0   0.0     0.0    0.0      0.0    0.0         0.0      0.0   0.0   \n",
      "1   0.0   0.0     0.0    0.0      0.0    0.0         0.0      0.0   0.0   \n",
      "2   0.0   0.0     0.0    0.0      0.0    0.0         0.0      0.0   0.0   \n",
      "3   0.0   0.0     0.0    0.0      0.0    0.0         0.0      0.0   0.0   \n",
      "4   0.0   0.0     0.0    0.0      0.0    0.0         0.0      0.0   0.0   \n",
      "\n",
      "   artichoke  ...  egg  fish  gluten  milk  nuts  peanuts  shellfish  soy  \\\n",
      "0        0.0  ...  0.0   0.0     0.0   0.0   0.0      0.0        0.0  1.0   \n",
      "1        0.0  ...  0.0   0.0     0.0   0.0   0.0      0.0        0.0  0.0   \n",
      "2        0.0  ...  0.0   0.0     1.0   1.0   0.0      0.0        0.0  1.0   \n",
      "3        0.0  ...  0.0   1.0     0.0   1.0   0.0      0.0        0.0  0.0   \n",
      "4        0.0  ...  0.0   0.0     0.0   0.0   0.0      0.0        0.0  1.0   \n",
      "\n",
      "   choking_hazards  hypoallergenic  \n",
      "0              0.0             0.0  \n",
      "1              0.0             1.0  \n",
      "2              0.0             0.0  \n",
      "3              1.0             0.0  \n",
      "4              0.0             0.0  \n",
      "\n",
      "[5 rows x 421 columns]\n"
     ]
    }
   ],
   "source": [
    "# Create a list of all feature names\n",
    "all_feature_names = (\n",
    "    list(tfidf_vectorizer.get_feature_names_out()) + \n",
    "    list(tags_df.columns) + \n",
    "    list(allergen_df.columns) + \n",
    "    ['choking_hazards', 'hypoallergenic']\n",
    ")\n",
    "\n",
    "print(f\"Total number of features: {len(all_feature_names)}\")\n",
    "print(f\"Feature names: {all_feature_names[:10]}...\")  # Show first 10 feature names\n",
    "\n",
    "# Convert sparse matrix to DataFrame (caution: may be large!)\n",
    "final_features_df = pd.DataFrame(\n",
    "    final_features.toarray(),  # Convert to dense array\n",
    "    columns=all_feature_names  # Use the combined feature names list\n",
    ")\n",
    "\n",
    "# Print the first few rows\n",
    "print(\"\\nFeature matrix shape:\", final_features_df.shape)\n",
    "print(\"\\nFirst few rows of feature matrix:\")\n",
    "print(final_features_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "aa4426de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of features: 421\n",
      "Feature names: ['abon', 'agar', 'almond', 'ambon', 'anchovy', 'apple', 'applesauce', 'apricot', 'aron', 'artichoke']...\n",
      "   abon  agar  almond  ambon  anchovy  apple  applesauce  apricot  aron  \\\n",
      "0   0.0   0.0     0.0    0.0      0.0    0.0         0.0      0.0   0.0   \n",
      "1   0.0   0.0     0.0    0.0      0.0    0.0         0.0      0.0   0.0   \n",
      "2   0.0   0.0     0.0    0.0      0.0    0.0         0.0      0.0   0.0   \n",
      "3   0.0   0.0     0.0    0.0      0.0    0.0         0.0      0.0   0.0   \n",
      "4   0.0   0.0     0.0    0.0      0.0    0.0         0.0      0.0   0.0   \n",
      "\n",
      "   artichoke  ...  egg  fish  gluten  milk  nuts  peanuts  shellfish  soy  \\\n",
      "0        0.0  ...  0.0   0.0     0.0   0.0   0.0      0.0        0.0  1.0   \n",
      "1        0.0  ...  0.0   0.0     0.0   0.0   0.0      0.0        0.0  0.0   \n",
      "2        0.0  ...  0.0   0.0     1.0   1.0   0.0      0.0        0.0  1.0   \n",
      "3        0.0  ...  0.0   1.0     0.0   1.0   0.0      0.0        0.0  0.0   \n",
      "4        0.0  ...  0.0   0.0     0.0   0.0   0.0      0.0        0.0  1.0   \n",
      "\n",
      "   choking_hazards  hypoallergenic  \n",
      "0              0.0             0.0  \n",
      "1              0.0             1.0  \n",
      "2              0.0             0.0  \n",
      "3              1.0             0.0  \n",
      "4              0.0             0.0  \n",
      "\n",
      "[5 rows x 421 columns]\n",
      "Complete cosine similarity shape: (520, 520)\n",
      "TF-IDF only cosine similarity shape: (520, 520)\n",
      "\n",
      "Complete feature similarity for first recipe (first 10): [1.         0.68018868 0.67091081 0.54545455 0.90909091 0.78804874\n",
      " 0.72953606 0.78619041 0.69159679 0.68670257]\n",
      "TF-IDF only similarity for first recipe (first 10): [1.         0.1338791  0.03657197 0.         0.         0.05399068\n",
      " 0.02489664 0.03264014 0.25352831 0.20219734]\n",
      "\n",
      "Top 5 most similar recipes to the first recipe (using all features):\n",
      "Recipe 0: Similarity = 1.0000\n",
      "  Food name: Edamame and Sweet Potato Dumplings (Oyaki)\n",
      "\n",
      "Recipe 445: Similarity = 0.9226\n",
      "  Food name: Duck Ungkep Rice (Nasi Bebek Ungkep)\n",
      "\n",
      "Recipe 4: Similarity = 0.9091\n",
      "  Food name: Miso Soup\n",
      "\n",
      "Recipe 399: Similarity = 0.9091\n",
      "  Food name: Steamed Tofu (Tahu Kukus)\n",
      "\n",
      "Recipe 421: Similarity = 0.9091\n",
      "  Food name: Steamed Milkfish (Tim Cah Ikan Kembung)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a list of all feature names\n",
    "all_feature_names = (\n",
    "    list(tfidf_vectorizer.get_feature_names_out()) + \n",
    "    list(tags_df.columns) + \n",
    "    list(allergen_df.columns) + \n",
    "    ['choking_hazards', 'hypoallergenic']\n",
    ")\n",
    "\n",
    "print(f\"Total number of features: {len(all_feature_names)}\")\n",
    "print(f\"Feature names: {all_feature_names[:10]}...\")  # Show first 10 feature names\n",
    "\n",
    "final_features_df = pd.DataFrame(\n",
    "    final_features.toarray(),  # Convert to dense array\n",
    "    columns=all_feature_names  # Use the combined feature names list\n",
    ")\n",
    "\n",
    "# Print the first few rows\n",
    "print(final_features_df.head())\n",
    "\n",
    "# Calculate cosine similarity using the complete feature matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Calculate similarity matrix using all features (not just TF-IDF)\n",
    "cosine_sim_complete = cosine_similarity(final_features, final_features)\n",
    "print(f\"Complete cosine similarity shape: {cosine_sim_complete.shape}\")\n",
    "\n",
    "# Also calculate similarity using just TF-IDF for comparison\n",
    "cosine_sim_tfidf = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "print(f\"TF-IDF only cosine similarity shape: {cosine_sim_tfidf.shape}\")\n",
    "\n",
    "# Show sample similarity scores for first recipe\n",
    "print(f\"\\nComplete feature similarity for first recipe (first 10): {cosine_sim_complete[0][:10]}\")\n",
    "print(f\"TF-IDF only similarity for first recipe (first 10): {cosine_sim_tfidf[0][:10]}\")\n",
    "\n",
    "# Find most similar recipes to the first recipe using complete features\n",
    "first_recipe_similarities = cosine_sim_complete[0]\n",
    "sorted_indices = np.argsort(first_recipe_similarities)[::-1]  # Sort in descending order\n",
    "\n",
    "print(\"\\nTop 5 most similar recipes to the first recipe (using all features):\")\n",
    "for i in range(5):\n",
    "    idx = sorted_indices[i]\n",
    "    similarity = first_recipe_similarities[idx]\n",
    "    print(f\"Recipe {idx}: Similarity = {similarity:.4f}\")\n",
    "    if 'food_name' in df.columns:\n",
    "        print(f\"  Food name: {df.iloc[idx]['food_name']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e8edfe92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF cosine similarity shape: (520, 520)\n",
      "TF-IDF cosine similarity for first recipe (first 5 values): [1.         0.1338791  0.03657197 0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "# Calculate cosine similarity using TF-IDF matrix only\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cosine_sim_tfidf = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "print(f\"TF-IDF cosine similarity shape: {cosine_sim_tfidf.shape}\")\n",
    "print(f\"TF-IDF cosine similarity for first recipe (first 5 values): {cosine_sim_tfidf[0][:5]}\")\n",
    "\n",
    "# Note: This is similarity based on TF-IDF features only\n",
    "# For complete similarity including dietary tags, allergens, and binary features,\n",
    "# see the next cell which uses the complete feature matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7459dc32",
   "metadata": {},
   "source": [
    "<h4><strong>Jaccard<strong></h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736a53cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c34e1c1f",
   "metadata": {},
   "source": [
    "<p><strong>Evaluation Metrics</strong><p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68320502",
   "metadata": {},
   "source": [
    "<p><strong>Confidence Level of Certain Recipe</strong></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ba90cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b6f8fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
